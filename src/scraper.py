"""
M√≥dulo principal de Web Scraping
"""
import time
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from loguru import logger
from tqdm import tqdm

from config import (
    BASE_URL, DELAY_BETWEEN_REQUESTS, SELECTORS, 
    USE_SELENIUM, USE_UNDETECTED_CHROMEDRIVER, SELENIUM_HEADLESS, SELENIUM_WAIT_TIME, SELENIUM_DRIVER
)
from src.utils import (
    safe_request, build_absolute_url, clean_text, 
    extract_price, sanitize_category, get_random_user_agent
)


class WebScraper:
    """Classe principal para fazer scraping de produtos"""
    
    def __init__(self, base_url: str = None):
        self.base_url = base_url or BASE_URL
        self.session = None
        self.products = []
        self.driver = None
        
        # Inicializa Selenium se necess√°rio
        if USE_SELENIUM:
            self._init_selenium()
    
    def _init_selenium(self):
        """Inicializa driver do Selenium"""
        try:
            # Tenta usar undetected-chromedriver primeiro (mais eficaz contra anti-bot)
            if USE_UNDETECTED_CHROMEDRIVER and SELENIUM_DRIVER.lower() == "chrome":
                try:
                    import undetected_chromedriver as uc
                    logger.info("Usando undetected-chromedriver (mais eficaz contra prote√ß√µes anti-bot)")
                    
                    options = uc.ChromeOptions()
                    if SELENIUM_HEADLESS:
                        options.add_argument('--headless=new')
                    options.add_argument('--no-sandbox')
                    options.add_argument('--disable-dev-shm-usage')
                    options.add_argument('--window-size=1920,1080')
                    
                    # Tenta inicializar com diferentes m√©todos
                    try:
                        self.driver = uc.Chrome(options=options, version_main=None, use_subprocess=True)
                    except:
                        try:
                            self.driver = uc.Chrome(options=options, version_main=None)
                        except Exception as e2:
                            logger.warning(f"Erro ao inicializar undetected-chromedriver: {e2}")
                            raise
                    
                    self.driver.implicitly_wait(SELENIUM_WAIT_TIME)
                    logger.info(f"undetected-chromedriver inicializado com sucesso (headless={SELENIUM_HEADLESS})")
                    return
                except ImportError as e:
                    logger.warning(f"undetected-chromedriver n√£o instalado: {e}")
                    logger.info("Execute: pip install undetected-chromedriver")
                    logger.info("Usando Selenium padr√£o...")
                except Exception as e:
                    logger.warning(f"Erro ao inicializar undetected-chromedriver: {e}")
                    logger.info("Tentando com Selenium padr√£o...")
            
            # Usa Selenium padr√£o
            from selenium import webdriver
            from selenium.webdriver.chrome.options import Options as ChromeOptions
            from selenium.webdriver.firefox.options import Options as FirefoxOptions
            
            # Tenta usar webdriver-manager se dispon√≠vel
            try:
                from webdriver_manager.chrome import ChromeDriverManager
                from webdriver_manager.firefox import GeckoDriverManager
                USE_WEBDRIVER_MANAGER = True
            except ImportError:
                USE_WEBDRIVER_MANAGER = False
                logger.info("webdriver-manager n√£o instalado. Usando ChromeDriver do sistema.")
            
            if SELENIUM_DRIVER.lower() == "chrome":
                options = ChromeOptions()
                if SELENIUM_HEADLESS:
                    options.add_argument('--headless=new')  # Novo modo headless
                options.add_argument('--no-sandbox')
                options.add_argument('--disable-dev-shm-usage')
                options.add_argument('--disable-blink-features=AutomationControlled')
                options.add_argument('--disable-gpu')
                options.add_argument('--window-size=1920,1080')
                options.add_experimental_option("excludeSwitches", ["enable-automation"])
                options.add_experimental_option('useAutomationExtension', False)
                options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
                
                # Prefs para parecer mais com navegador real
                prefs = {
                    "credentials_enable_service": False,
                    "profile.password_manager_enabled": False
                }
                options.add_experimental_option("prefs", prefs)
                
                try:
                    if USE_WEBDRIVER_MANAGER:
                        from selenium.webdriver.chrome.service import Service
                        service = Service(ChromeDriverManager().install())
                        self.driver = webdriver.Chrome(service=service, options=options)
                    else:
                        self.driver = webdriver.Chrome(options=options)
                except Exception as e:
                    logger.error(f"Erro ao iniciar Chrome: {e}")
                    logger.info("Instale o ChromeDriver ou execute: pip install webdriver-manager")
                    return
                    
            elif SELENIUM_DRIVER.lower() == "firefox":
                options = FirefoxOptions()
                if SELENIUM_HEADLESS:
                    options.add_argument('--headless')
                
                try:
                    if USE_WEBDRIVER_MANAGER:
                        from selenium.webdriver.firefox.service import Service
                        service = Service(GeckoDriverManager().install())
                        self.driver = webdriver.Firefox(service=service, options=options)
                    else:
                        self.driver = webdriver.Firefox(options=options)
                except Exception as e:
                    logger.error(f"Erro ao iniciar Firefox: {e}")
                    logger.info("Instale o GeckoDriver ou execute: pip install webdriver-manager")
                    return
            else:
                logger.error(f"Driver {SELENIUM_DRIVER} n√£o suportado. Use 'chrome' ou 'firefox'")
                return
                
            if self.driver:
                self.driver.implicitly_wait(SELENIUM_WAIT_TIME)
                logger.info(f"Selenium inicializado com {SELENIUM_DRIVER} (headless={SELENIUM_HEADLESS})")
                
        except ImportError:
            logger.error("Selenium n√£o instalado. Execute: pip install selenium")
            logger.info("Para usar requisi√ß√µes normais, defina USE_SELENIUM = False em config.py")
        except Exception as e:
            logger.error(f"Erro ao inicializar Selenium: {e}")
    
    def _get_page_selenium(self, url: str) -> Optional[str]:
        """Obt√©m HTML usando Selenium"""
        if not self.driver:
            logger.error("Driver Selenium n√£o inicializado")
            return None
        
        try:
            # Executa script para remover indicadores de automa√ß√£o
            self.driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {
                'source': '''
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => undefined
                    });
                    window.navigator.chrome = {
                        runtime: {},
                    };
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [1, 2, 3, 4, 5],
                    });
                '''
            })
            
            self.driver.get(url)
            
            # Aguarda o carregamento completo da p√°gina
            time.sleep(5)  # Aguarda JavaScript carregar
            
            # Tenta aguardar at√© que algum elemento da p√°gina esteja presente
            try:
                from selenium.webdriver.support.ui import WebDriverWait
                from selenium.webdriver.support import expected_conditions as EC
                from selenium.webdriver.common.by import By
                
                # Aguarda at√© que o body esteja presente (m√≠nimo 30 segundos)
                WebDriverWait(self.driver, 30).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
            except:
                pass  # Se n√£o conseguir esperar, continua mesmo assim
            
            # Verifica se a p√°gina carregou corretamente (n√£o √© p√°gina de erro)
            page_source = self.driver.page_source
            
            # Se ainda retornar 403, tenta mais uma vez com delay maior
            if "403" in page_source and "Forbidden" in page_source:
                logger.warning("P√°gina ainda retorna 403. Tentando aguardar mais tempo...")
                time.sleep(15)  # Aguarda mais tempo
                
                # Tenta recarregar a p√°gina
                try:
                    self.driver.refresh()
                    time.sleep(10)
                    page_source = self.driver.page_source
                except:
                    pass
                
                # Se ainda estiver bloqueado, retorna None para pular esta URL
                if "403" in page_source and "Forbidden" in page_source:
                    logger.error(f"‚ùå Acesso bloqueado (403) para {url}")
                    logger.info("üí° O site est√° bloqueando acesso automatizado. Poss√≠veis solu√ß√µes:")
                    logger.info("   1. Use navegador manual e salve o HTML")
                    logger.info("   2. Aguarde alguns minutos e tente novamente")
                    logger.info("   3. Verifique se precisa fazer login no site")
                    return None  # Retorna None para pular esta URL
            
            return page_source
        except Exception as e:
            logger.error(f"Erro ao acessar {url} com Selenium: {e}")
            return None
        
    def get_page(self, url: str) -> Optional[BeautifulSoup]:
        """
        Obt√©m e faz parse de uma p√°gina HTML
        
        Args:
            url: URL da p√°gina
            
        Returns:
            BeautifulSoup object ou None
        """
        url = build_absolute_url(self.base_url, url)
        
        html_content = None
        
        if USE_SELENIUM and self.driver:
            # Usa Selenium
            html_content = self._get_page_selenium(url)
        else:
            # Usa requisi√ß√£o HTTP normal
            response = safe_request(url)
            if response:
                html_content = response.content
            else:
                # Se falhar e Selenium n√£o estiver habilitado, sugere usar Selenium
                if not USE_SELENIUM:
                    logger.warning(f"Falha ao acessar {url}. O site pode requerer JavaScript.")
                    logger.info("Considere habilitar Selenium em config.py: USE_SELENIUM = True")
        
        if not html_content:
            return None
        
        try:
            soup = BeautifulSoup(html_content, 'lxml')
            return soup
        except Exception as e:
            logger.error(f"Erro ao fazer parse da p√°gina {url}: {e}")
            return None
    
    def __del__(self):
        """Fecha driver do Selenium ao destruir objeto"""
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
    
    def extract_product_info(self, product_element, base_url: str = "") -> Dict:
        """
        Extrai informa√ß√µes de um produto de um elemento HTML
        
        Args:
            product_element: Elemento BeautifulSoup do produto
            base_url: URL base para construir URLs absolutas
            
        Returns:
            Dicion√°rio com informa√ß√µes do produto
        """
        base_url = base_url or self.base_url
        
        # Extrai informa√ß√µes usando seletores
        selectors = SELECTORS
        
        try:
            # Nome do produto
            name_elem = product_element.select_one(selectors.get('product_name', '')) if selectors.get('product_name') else None
            name = clean_text(name_elem.get_text()) if name_elem else ""
            
            # Pre√ßo
            price_elem = product_element.select_one(selectors.get('product_price', '')) if selectors.get('product_price') else None
            price_text = clean_text(price_elem.get_text()) if price_elem else ""
            price = extract_price(price_text)
            
            # Imagem
            image_elem = product_element.select_one(selectors.get('product_image', '')) if selectors.get('product_image') else None
            image_url = ""
            if image_elem:
                image_url = image_elem.get('src') or image_elem.get('data-src') or image_elem.get('data-lazy-src') or ""
            image_url = build_absolute_url(base_url, image_url)
            
            # Link do produto
            link_elem = product_element.select_one(selectors.get('product_link', '')) if selectors.get('product_link') else None
            link = ""
            if link_elem:
                link = link_elem.get('href') or ""
            link = build_absolute_url(base_url, link)
            
            # Categoria
            category_elem = product_element.select_one(selectors.get('product_category', '')) if selectors.get('product_category') else None
            category = clean_text(category_elem.get_text()) if category_elem else "Sem_Categoria"
            category = sanitize_category(category)
            
            # Gera ID √∫nico (pode ser melhorado)
            product_id = f"{category}_{name}" if name else f"produto_{time.time()}"
            product_id = product_id.replace(' ', '_')[:100]
            
            product_info = {
                'id': product_id,
                'nome': name,
                'categoria': category,
                'preco': price,
                'preco_original': price_text,  # Mant√©m formato original
                'imagem_url': image_url,
                'link': link,
                'data_coleta': time.strftime("%Y-%m-%d %H:%M:%S")
            }
            
            return product_info
            
        except Exception as e:
            logger.error(f"Erro ao extrair informa√ß√µes do produto: {e}")
            return {}
    
    def scrape_category_page(self, category_url: str) -> List[Dict]:
        """
        Faz scraping de uma p√°gina de categoria
        
        Args:
            category_url: URL da p√°gina de categoria
            
        Returns:
            Lista de produtos encontrados
        """
        logger.info(f"Scraping p√°gina: {category_url}")
        
        soup = self.get_page(category_url)
        if not soup:
            return []
        
        products = []
        
        # Encontra containers de produtos
        container_selector = SELECTORS.get('product_container', '')
        if not container_selector:
            logger.warning("Seletor de container de produtos n√£o configurado!")
            return []
        
        product_containers = soup.select(container_selector)
        logger.info(f"Encontrados {len(product_containers)} produtos")
        
        for container in tqdm(product_containers, desc="Extraindo produtos"):
            try:
                product_info = self.extract_product_info(container, self.base_url)
                if product_info and product_info.get('nome'):  # S√≥ adiciona se tiver nome
                    products.append(product_info)
            except Exception as e:
                logger.error(f"Erro ao processar produto: {e}")
                continue
        
        # Delay entre requisi√ß√µes
        if DELAY_BETWEEN_REQUESTS > 0:
            time.sleep(DELAY_BETWEEN_REQUESTS)
        
        return products
    
    def scrape_multiple_pages(self, category_url: str, max_pages: int = 1) -> List[Dict]:
        """
        Faz scraping de m√∫ltiplas p√°ginas de uma categoria
        
        Args:
            category_url: URL da primeira p√°gina
            max_pages: N√∫mero m√°ximo de p√°ginas para processar
            
        Returns:
            Lista de todos os produtos encontrados
        """
        all_products = []
        current_url = category_url
        page = 1
        
        while page <= max_pages:
            logger.info(f"Processando p√°gina {page}/{max_pages}")
            
            products = self.scrape_category_page(current_url)
            all_products.extend(products)
            
            # Tenta encontrar link para pr√≥xima p√°gina
            if page < max_pages:
                soup = self.get_page(current_url)
                if soup:
                    next_selector = SELECTORS.get('next_page', '')
                    if next_selector:
                        next_link = soup.select_one(next_selector)
                        if next_link:
                            next_href = next_link.get('href')
                            if next_href:
                                current_url = build_absolute_url(self.base_url, next_href)
                                page += 1
                                continue
            
            # Se n√£o encontrou pr√≥xima p√°gina, para
            break
        
        logger.info(f"Total de produtos coletados: {len(all_products)}")
        return all_products
    
    def scrape_categories(self, category_urls: List[str], max_pages_per_category: int = 1) -> List[Dict]:
        """
        Faz scraping de m√∫ltiplas categorias
        
        Args:
            category_urls: Lista de URLs de categorias
            max_pages_per_category: N√∫mero m√°ximo de p√°ginas por categoria
            
        Returns:
            Lista de todos os produtos encontrados
        """
        all_products = []
        
        for category_url in category_urls:
            logger.info(f"Processando categoria: {category_url}")
            products = self.scrape_multiple_pages(category_url, max_pages_per_category)
            all_products.extend(products)
            
            # Delay entre categorias
            if DELAY_BETWEEN_REQUESTS > 0:
                time.sleep(DELAY_BETWEEN_REQUESTS * 2)
        
        # Remove duplicatas baseado no ID
        unique_products = []
        seen_ids = set()
        for product in all_products:
            product_id = product.get('id')
            if product_id and product_id not in seen_ids:
                seen_ids.add(product_id)
                unique_products.append(product)
        
        logger.info(f"Total de produtos √∫nicos coletados: {len(unique_products)}")
        return unique_products

